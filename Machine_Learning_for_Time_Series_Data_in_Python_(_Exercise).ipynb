{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf-gpu-2.0",
      "language": "python",
      "name": "tf_gpu2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Machine_Learning_for_Time_Series_Data_in_Python_(_Exercise).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Time-Series-Analysis-in-Python/blob/master/Machine_Learning_for_Time_Series_Data_in_Python_(_Exercise).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLpgOW1eIvq0"
      },
      "source": [
        "1 - Time Series and Machine Learning Primer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U2u5S2IIvq1"
      },
      "source": [
        "!git clone https://github.com/hussain0048/Time-Series-Analysis-in-Python.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8im90Q8skxno",
        "outputId": "28fa658a-7573-4c02-9f42-4733f4b894f7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# this code is used to upload dataset from Pc to colab\n",
        "from google.colab import files # Please First run this cod in chrom \n",
        "def getLocalFiles():\n",
        "    _files = files.upload() # upload StudentNextSessionf.csv datase\n",
        "    if len(_files) >0: # Then run above  libray \n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0409be0d-272a-4f52-b47f-5895415c068b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0409be0d-272a-4f52-b47f-5895415c068b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_cases1.csv to data_cases1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VEjJRNCk2ua"
      },
      "source": [
        "# **Important libray**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5O4oYyplCsN"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzpVoEZelN_w"
      },
      "source": [
        "data = pd.read_csv('data_cases1.csv', usecols=['Date','Confirmed'],index_col='Date', parse_dates=True )\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg2QmYCfkBOk"
      },
      "source": [
        "#**1-Time Series and Machine Learning Primer**\n",
        "This chapter is an introduction to the basics of machine learning, time series data, and the intersection between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7IBX3AeFhrM"
      },
      "source": [
        "## **1.1 Time sieres kinds and applications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwnIbf9qkKxN"
      },
      "source": [
        "### **1.1.1 Plotting a time series (1)**\n",
        "\n",
        "In this exercise, you'll practice plotting the values of two time series without the time component.\n",
        "Two DataFrames, data and data2 are available in your workspace.\n",
        "\n",
        "Unless otherwise noted, assume that all required packages are loaded with their common aliases throughout this course.\n",
        "\n",
        "Note: This course assumes some familiarity with time series data, as well as how to use them in data analytics pipelines. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgm_ekOZl_Qc"
      },
      "source": [
        "# Print the first 5 rows of data\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNGxKkhiIvq5"
      },
      "source": [
        "# Plot the time series in each dataset\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
        "data.iloc[:1000].plot(y='data_values', ax=axs[0])\n",
        "data2.iloc[:1000].plot(y='data_values', ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9EFJEv9yeD"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ESb3vBxskUApTHJGXwbbuvyjOrSVVFdw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiTGnlbtvbZP"
      },
      "source": [
        "### **1.1.2 Plotting a time series (II)**\n",
        "\n",
        "You'll now plot both the datasets again, but with the included time stamps for each (stored in the column called \"time\"). Let's see if this gives you some more context for understanding each time series data.\n",
        "\n",
        "Plot data and data2 on top of one another, one per axis object.\n",
        "\n",
        "The x-axis should represent the time stamps and the y-axis should represent the dataset values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pQNfc-1Ivq-"
      },
      "source": [
        "# Plot the time series in each dataset\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
        "data.iloc[:1000].plot(x='time', y='data_values', ax=axs[0])\n",
        "data.iloc[:1000].plot(x='time', y='data_values', ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEEowo3G_Ak5"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1HetuK2uXmyfNsWOA7Tg0iWAqvvPwG-_z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NPuMA2dMWuW"
      },
      "source": [
        "## **1.2-Machine Learning Basic**\n",
        "\n",
        "**Scikit-learn**\n",
        "\n",
        "is most popular libary of machine leanring \n",
        "\n",
        "**Preparing data for scikit-learn**\n",
        "- scikit-learn expects a particular structure of data:\n",
        "(samples, features)\n",
        "- Make sure that your data is atleasttwo-dimensiona\n",
        "- Make sure the rst dimension is samples\n",
        "\n",
        "**If your data is not shaped properly**\n",
        "\n",
        "Ifthe axes are swapped:\n",
        "- array.T.shape\n",
        "\n",
        "**If your data is not shaped properly**\n",
        "\n",
        "If we're missing an axis, use .reshape() :\n",
        "- array.shape\n",
        "- (10,)\n",
        "- rray.reshape([-1, 1]).shape\n",
        "- (10, 1)\n",
        "- -1 will automatically llthat axis with remaining values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-_QOyLQEn6"
      },
      "source": [
        "### **1.2.1 Fitting a simple model: classification**\n",
        "In this exercise, you'll use the iris dataset (representing petal characteristics of a number of flowers) to practice using the scikit-learn API to fit a classification model. You can see a sample plot of the data to the right.\n",
        "\n",
        "Extract the \"petal length (cm)\" and \"petal width (cm)\" columns of data and assign it to X.\n",
        "\n",
        "Fit a model on X and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amjc1WnlIvrC"
      },
      "source": [
        "# Print the first 5 rows for inspection\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnNOlBbAIvrE"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "# Construct data for the model\n",
        "X = data[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = data[['target']]\n",
        "\n",
        "# Fit the model\n",
        "model = LinearSVC()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbGIecTKNpWB"
      },
      "source": [
        "### **1.2.2 Predicting using a classification model**\n",
        "\n",
        "Now that you have fit your classifier, let's use it to predict the type of flower (or class) for some newly-collected flowers.\n",
        "\n",
        "Information about petal width and length for several new flowers is stored in the variable targets. Using the classifier you fit, you'll predict the type of each flower.\n",
        "\n",
        "Predict the flower type using the array X_predict.\n",
        "Run the given code to visualize the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K5fNIaHIvrK"
      },
      "source": [
        "# Create input array\n",
        "X_predict = targets[['petal length (cm)', 'petal width (cm)']]\n",
        "\n",
        "# Predict with the model\n",
        "predictions = model.predict(X_predict)\n",
        "print(predictions)\n",
        "\n",
        "# Visualize predictions and actual values\n",
        "plt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n",
        "            c=predictions, cmap=plt.cm.coolwarm)\n",
        "plt.title(\"Predicted class values\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf1G9JIXPuAu"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1TcY2C2h-JI5X2Rs2wKLVnLY9hnnApZDu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3FA0-1oP6uF"
      },
      "source": [
        "### **1.2.3-Fitting a simple model: regression**\n",
        "\n",
        "In this exercise, you'll practice fitting a regression model using data from the Boston housing market. A DataFrame called boston is available in your workspace. It contains many variables of data (stored as columns). Can you find a relationship between the following two variables?\n",
        "\n",
        "- \"AGE\": proportion of owner-occupied units built prior to 1940\n",
        "- \"RM\" : average number of rooms per dwelling\n",
        "\n",
        "Prepare X and y DataFrames using the data in boston.\n",
        "X should be the proportion of houses built prior to 1940, y average number of rooms per dwelling.\n",
        "Fit a regression model that uses these variables (remember to shape the variables correctly!).\n",
        "Don't forget that each variable must be the correct shape for scikit-learn to use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpDrBD7IIvrP"
      },
      "source": [
        "from sklearn import linear_model\n",
        "# Prepare input and output DataFrames\n",
        "X = boston[['AGE']]\n",
        "y = boston[['RM']]\n",
        "# Fit the model\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CZiNxkkRdfQ"
      },
      "source": [
        "### **1.2.4 Predicting using a regression model**\n",
        "\n",
        "Now that you've fit a model with the Boston housing data, lets see what predictions it generates on some new data. You can investigate the underlying relationship that the model has found between inputs and outputs by feeding in a range of numbers as inputs and seeing what the model predicts for each input.\n",
        "\n",
        "A 1-D array new_inputs consisting of 100 \"new\" values for \"AGE\" (proportion of owner-occupied units built prior to 1940) is available in your workspace along with the model you fit in the previous exercise.\n",
        "\n",
        "Review new_inputs in the shell.\n",
        "Reshape new_inputs appropriately to generate predictions.\n",
        "Run the given code to visualize the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgwo-0xgIvrW"
      },
      "source": [
        "# Generate predictions with the model using those inputs\n",
        "predictions = model.predict(new_inputs.reshape(-1,1))\n",
        "# Visualize the inputs and predicted values\n",
        "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
        "plt.xlabel('inputs')\n",
        "plt.ylabel('predictions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InnbPcDSSdQ"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1flKX2T_99HNRO_soOf5RgFnjP6bJs6_o)\n",
        "\n",
        "Here the red line shows the relationship that your model found. As the proportion of pre-1940s houses gets larger, the average number of rooms gets slightly lower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr1CtH3aazAR"
      },
      "source": [
        "## **1.3 Machine learning and time series data**\n",
        "\n",
        "**Getting to know our data**\n",
        "- The datasets that we'll use in this course are all freely-available online\n",
        "- There are many datasets available to download on the web,the ones we'll use come from Kaggle\n",
        "\n",
        "**The Heartbeat Acoustic Data**\n",
        "\n",
        "- Many recordings of heart sounds from different patients\n",
        "- Some had normally-functioning hearts, others had abnormalities\n",
        "- Data comes in the form of audio les + labels for each le\n",
        "- Can we nd the \"abnormal\" heart beats?\n",
        "\n",
        "**Loading auditory data**\n",
        "\n",
        "Audo data mostly store in way file. The glob function is used to listen this file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jK2IsDzIvrZ"
      },
      "source": [
        "from glob import glob\n",
        "files = glob('data/heartbeat-sounds/files/*.wav')\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdWQ0OQNdiML"
      },
      "source": [
        "### **1.3.1 Reading in auditory data**\n",
        "\n",
        "we used **librosa** read the audio dataset and it also have function to extraction features, visiluzation and analysis for audio data. The data is load through **load** function. The data are stroe in **audo** and **sfreq** . The sfreq is 2205 which means there is 2205 samples recoreded per second "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1GkQs5ofABR"
      },
      "source": [
        "import librosa as lr\n",
        "# `load` accepts a path to an audio file\n",
        "audio, sfreq = lr.load('data/heartbeat-sounds/proc/files/murmur__201101051104.wav')\n",
        "print(sfreq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs4-JCUVfvSS"
      },
      "source": [
        "###**1.3.2-Inferring time from samples**\n",
        "\n",
        "- If we know the sampling rate of a timeseries,then we know the timestamp of each datapoint relative\n",
        "to the rst datapoint\n",
        "- Note:this assumes the sampling rate is xed and no data points are lost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ne9sv60gt0S"
      },
      "source": [
        "### **1.3.3 Creating a time array (I)**\n",
        "\n",
        "Now we create array of timestammp for data . to do so you have two option 1) Generate array of indice from zero to number of data point in your audio file, divide each indix with sampling frequencey and finally you will get timepoint for each data point "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybfzgsHah8jP"
      },
      "source": [
        "indices = np.arange(0, len(audio))\n",
        "time = indices / sfreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wRRoHjfYn-i"
      },
      "source": [
        "### **1.3.4 Creating a time array (II)**\n",
        "Find the time stamp for theN-1th data point. Then use linspace() to interpolate from zero to\n",
        "that time\n",
        "or Cacluate the final time point of your audio data using similar method, then used the linspace function even space number between zero to final time points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3iSL1wsZiiv"
      },
      "source": [
        "final_time = (len(audio) - 1) / sfreq\n",
        "time = np.linspace(0, final_time, sfreq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56BmQGhYaEQX"
      },
      "source": [
        "### **1.3.5 The New York Stock Exchange dataset**\n",
        "- This dataset consists of company stock values for 10 years\n",
        "- Can we detect any patterns in historical records that allow us to predict the value of companies in the\n",
        "future?\n",
        "- it has one sample per day \n",
        "- Since we predict continuse value therefore it is regression problem \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z24afmBMbFPa"
      },
      "source": [
        "### **1.3.6 Looking atthe data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ObOCDv4bOMs"
      },
      "source": [
        "data = pd.read_csv('path/to/data.csv')\n",
        "data.columns\n",
        "#Index(['date', 'symbol', 'close', 'volume'], dtype='object')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E1xGkhVbkeB"
      },
      "source": [
        "### **1.3.7 Timeseries with Pandas DataFrames**\n",
        "We can investigate the object type of each column by accessing the dtypes attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUlrbYabcGbH"
      },
      "source": [
        "df['date'].dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm5r0kaEcKhm"
      },
      "source": [
        "### **1.3.8 Converting a column to a time series**\n",
        "To ensure that a column within a DataFrame is treated as time series, use the to_datetime()\n",
        "function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhwu1PbvcpZt"
      },
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['date']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o19gPBW4dLdN"
      },
      "source": [
        "###**1.3.9 Inspecting the classification data (exercise)**\n",
        "\n",
        "In these final exercises of this chapter, you'll explore the two datasets you'll use in this course.\n",
        "\n",
        "The first is a collection of heartbeat sounds. Hearts normally have a predictable sound pattern as they beat, but some disorders can cause the heart to beat abnormally. This dataset contains a training set with labels for each type of heartbeat, and a testing set with no labels. You'll use the testing set to validate your models.\n",
        "\n",
        "As you have labeled data, this dataset is ideal for classification. In fact\n",
        "it was originally offered as a part of a[Kaggle competition](https://www.kaggle.com/kinguistics/heartbeat-sounds)\n",
        "\n",
        "- Use glob to return a list of the .wav files in data_dir directory.\n",
        "- Import the first audio file in the list using librosa.\n",
        "- Generate a time array for the data.\n",
        "- Plot the waveform for this file, along with the time array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iicq-0kRdaA_"
      },
      "source": [
        "import librosa as lr\n",
        "from glob import glob\n",
        "\n",
        "# List all the wav files in the folder\n",
        "audio_files = glob(data_dir + '/*.wav')\n",
        "\n",
        "# Read in the first audio file, create the time array\n",
        "audio, sfreq = lr.load(audio_files[0])\n",
        "time = np.arange(0, len(audio)) / sfreq\n",
        "\n",
        "# Plot audio over time\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(time, audio)\n",
        "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
        "plt.show()\n",
        "\n",
        "#A common procedure in machine learning is to separate the\n",
        "# datapoints with lots of stuff happening from the ones that don't."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H-7BAOuJ8Nh"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1m8GJUEmlGgAEVYb_JJgPegwG5g5CMjH3\n",
        ")\n",
        "\n",
        "Good job! There are several seconds of heartbeat sounds in here, though note that most of this time is silence. A common procedure in machine learning is to separate the datapoints with lots of stuff happening from the ones that don't."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czcA8-ffg2he"
      },
      "source": [
        "### **1.3.10 Inspecting the regression data**\n",
        "\n",
        "The next dataset contains information about company market value over several years of time. This is one of the most popular kind of time series data used for regression. If you can model the value of a company as it changes over time, you can make predictions about where that company will be in the future. This dataset was also originally provided as part of a [public Kaggle competition](https://www.kaggle.com/dgawlik/nyse).\n",
        "\n",
        "In this exercise, you'll plot the time series for a number of companies to get an understanding of how they are (or aren't) related to one another.\n",
        "\n",
        "- Import the data with Pandas (stored in the file 'prices.csv').\n",
        "- Convert the index of data to datetime.\n",
        "- Loop through each column of data and plot the the column's values over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9Y2P5bKIvrl"
      },
      "source": [
        "# Read in the data\n",
        "data = pd.read_csv('prices.csv', index_col=0)\n",
        "# Convert the index of the DataFrame to datetime\n",
        "data.index = pd.to_datetime(data.index)\n",
        "print(data.head())\n",
        "________________________________________\n",
        "# Loop through each column, plot its values over time\n",
        "fig, ax = plt.subplots()\n",
        "for column in data.columns:\n",
        "    data[column].plot(ax=ax, label=column)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP1xf5hvOBnj"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1rmz7lUaH23WX4MSoKcspUncoOzETxjYW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XStlCSu8MZLk"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=11kakjV08AlXrcmVRy6cnX6iRRvwZIrN4)\n",
        "\n",
        "Good job! Note that each company's value is sometimes correlated with others, and sometimes not. Also note there are a lot of 'jumps' in there - what effect do you think these jumps would have on a predictive model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN2ekKZ-dkEO"
      },
      "source": [
        "#**2-Time Series as Inputs to a Model**\n",
        "\n",
        "The easiest way to incorporate time series into your machine learning pipeline is to use them as features in a model. This chapter covers common features that are extracted from time series in order to do machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au7jbsZ9Z7Kj"
      },
      "source": [
        "##**2.1Classifying a time series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR_iVCafWckk"
      },
      "source": [
        "**Visualize your time series data**\n",
        "\n",
        "To plot raw audio , we need two things, the raw audio waveform, usually in a1- or 2- dimensional array. We also need timepoint of each sample. We can calculate the time by dividing the index of each sampel  by the sampling freqency of the timeseries.This give us the time for each sample relative to the begining of the audio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS01I_WfWqxy"
      },
      "source": [
        "ixs = np.arange(audio.shape[-1])\n",
        "time = ixs / sfreq\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(time, audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3NfzNWOX1JI"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1BtrJwO18yHoScC0dcltVhSuKrMXom10b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9cRgMtcX9M6"
      },
      "source": [
        " **What features to use?**\n",
        "\n",
        "Using raw data as input to classifier is usually to noisy to be useful. an easy first step is to calculate summary statistics of our data,which removes the time dimension and give us a more traditional classification dataset.\n",
        "\n",
        "For each timesereis, we calcuate several summary statistics.These then can be used as features for model we have expanded a single feature (raw audio amplitude) to several features(here, the min, max and  average of each sample). Here we show how to calculate multiple feature for a several timeseries "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnZD-YFnZgWC"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1WpeD283O_2plMOUPaf-ZuWewzMjrumlm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzlv-0xVlXbb"
      },
      "source": [
        "**Calculating multiple featurs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Anr8uESIvro"
      },
      "source": [
        "print(audio.shape)\n",
        "# (n_files, time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq7fqkqvljNA"
      },
      "source": [
        "means = np.mean(audio, axis=-1)\n",
        "maxs = np.max(audio, axis=-1)\n",
        "stds = np.std(audio, axis=-1)\n",
        "print(means.shape)\n",
        "# (n_files,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHHE0Q3AmUSn"
      },
      "source": [
        "By using, the \"axis equal -1\" keyword, we collaps across the last dimension, which is time , The result is an array of number, one per time series "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxSxelslvlB"
      },
      "source": [
        "**Fitting a classier with scikit-learn**\n",
        "\n",
        "We collapsed a two-dimensional array into a one-dimensional for each feature of interest We can then combine these as inputs to a model. In case of  , we also need a label for each timeseries that allow us to build a classifier .\n",
        "\n",
        "In to order prepare your data for scikit-learn, remember to ensure that it has the correct shape,which is smaple by features. Here we can use the **column stack function** which let us stack array by turning them  into the columns of a two-dimensional array. In additional. The labels array is 1 dimensional, so we reshape it so that it is two dimensions. Finaly, we fit our model to These array, x, Y "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdl4xzL6ooQ7"
      },
      "source": [
        "# Import a linear classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "# Note that means are reshaped to work with scikit-learn\n",
        "X = np.column_stack([means, maxs, stds])\n",
        "y = labels.reshape([-1, 1])\n",
        "model = LinearSVC()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn5MsOT9o5uT"
      },
      "source": [
        "**Scoring your scikit-learn model**\n",
        "\n",
        "Now that we fit our model, we will score the classifier. There are many ways that we can score a classifier with scikit-learn. We can then calculate a score by dividing the total number of correct predictions by the total number of test sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-W9BU6rNvw"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Different input data\n",
        "predictions = model.predict(X_test)\n",
        "# Score our model with % correct\n",
        "# Manually\n",
        "percent_score = sum(predictions == labels_test) / len(labels_test)\n",
        "# Using a sklearn scorer\n",
        "percent_score = accuracy_score(labels_test, prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhxFk8mrwq7"
      },
      "source": [
        "### **2.1.1-Many repetitions of sounds**\n",
        "In this exercise, you'll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result.\n",
        "\n",
        "You'll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let's see if you can spot the difference.\n",
        "\n",
        "Two DataFrames, normal and abnormal, each with the shape of (n_times_points, n_audio_files) containing the audio for several heartbeats are available in your workspace. Also, the sampling frequency is loaded into a variable called sfreq. A convenience plotting function show_plot_and_make_titles() is also available in your workspace.\n",
        "\n",
        "- First, create the time array for these audio files (all audios are the same length).\n",
        "- Then, stack the values of the two DataFrames together (normal and abnormal, in that order) so that you have a single array of shape (n_audio_files, n_times_points).\n",
        "- Finally, use the code provided to loop through each list item / axis, and plot the audio over time in the corresponding axis object.\n",
        "You'll plot normal heartbeats in the left column, and abnormal ones in the right column\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KowOa-kKIvrx"
      },
      "source": [
        "## Many repetitions of sounds\n",
        "\n",
        "fig, axs = plt.subplots(3, 2, figsize=(15, 7), \n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "# Calculate the time array\n",
        "time = np.arange(0, len(normal)) / sfreq\n",
        "\n",
        "# Stack the normal/abnormal audio so you can loop and plot\n",
        "stacked_audio = np.hstack([normal, abnormal]).T\n",
        "\n",
        "# Loop through each audio file / ax object and plot\n",
        "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
        "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
        "    ax.plot(time, iaudio)\n",
        "show_plot_and_make_titles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PPVtY1d-ao-"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1_Y1AzKt0c77EmEtv08NwwWKbmjWnqvDy)\n",
        "\n",
        "Correct! As you can see there is a lot of variability in the raw data, let's see if you can average out some of that noise to notice a difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq5Hd9N3_X7p"
      },
      "source": [
        "###**2.1.2 Invariance in time**\n",
        "\n",
        "While you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren't discoverable by the naked eye.\n",
        "\n",
        "Another common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not).\n",
        "\n",
        "In this exercise, you'll average across many instances of each class of heartbeat sound.\n",
        "\n",
        "The two DataFrames (normal and abnormal) and the time array (time) from the previous exercise are available in your workspace.\n",
        "\n",
        "Average across the audio files contained in normal and abnormal, leaving the time dimension.\n",
        "Visualize these averages over time.\n",
        "\n",
        "**Hint**\n",
        "- In this case, the different timeseries correspond to index 1.\n",
        "- The audio amplitude should be on the y-axis, and time on the x-axis.\n",
        "- Make sure you are plotting just the averages!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctDdF258Ivr2"
      },
      "source": [
        "## Invariance in time\n",
        "\n",
        "# Average across the audio files of each DataFrame\n",
        "mean_normal = np.mean(normal, axis=1)\n",
        "mean_abnormal = np.mean(abnormal, axis=1)\n",
        "\n",
        "# Plot each average over time\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
        "ax1.plot(time, mean_normal)\n",
        "ax1.set(title=\"Normal Data\")\n",
        "ax2.plot(time, mean_abnormal)\n",
        "ax2.set(title=\"Abnormal Data\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVxRrUO7S6j"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=11CIQ6gtBdltaC2H2N1WBg-wvnjU8zBWL)\n",
        "\n",
        "Correct! Do you see a noticeable difference between the two? Maybe, but it's quite noisy. Let's see how you can dig into the data a bit further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZHlJv_n80iU"
      },
      "source": [
        "### **2.1.3 Build a classification model**\n",
        "\n",
        "While eye-balling differences is a useful way to gain an intuition for the data, let's see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data.\n",
        "\n",
        "We've split the two DataFrames (normal and abnormal) into X_train, X_test, y_train, and y_test.\n",
        "\n",
        "**Instruction**\n",
        "\n",
        "- Create an instance of the Linear SVC model and fit the model using the training data.\n",
        "- Use the testing data to generate predictions with the model.\n",
        "- Score the model using the provided code.\n",
        "\n",
        "**Hint**\n",
        "\n",
        "- Call LinearSVC() without arguments to define the model.\n",
        "- Call model.fit() on X_train and y_train to fit the model.\n",
        "- You can generate predictions using model.predict() on X_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaGuZDfGIvr6"
      },
      "source": [
        "## Build a classification model\n",
        "from sklearn.svm import LinearSVC\n",
        "# Initialize and fit the model\n",
        "model = LinearSVC()\n",
        "model.fit(X_train, y_train)\n",
        "# Generate predictions and score them manually\n",
        "predictions = model.predict(X_test)\n",
        "print(sum(predictions == y_test.squeeze()) / len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyYDyFJEIvr9"
      },
      "source": [
        "#OUTPUT\n",
        "0.555555555556"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKfwtAmC-owf"
      },
      "source": [
        "Correct! Note that your predictions didn't do so well. That's because the features you're using as inputs to the model (raw data) aren't very good at differentiating classes. Next, you'll explore how to calculate some more complex features that may improve the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rev4TTFxQ_bh"
      },
      "source": [
        "## **2.2  Improving features for classification**\n",
        "\n",
        "**The Auditory envelope**\n",
        "\n",
        "we will begin by calcuating the envelope of each heartbeat sound.\n",
        "The envelope throws away information about the fine-grained changes in the signal,focusing on the general shape of the audio waveform.To do this, we will need to calcualte the audio's amplitude , then it over time.\n",
        "\n",
        "**Smoothing over time**\n",
        "\n",
        "First, we will remove noise in timeseries data by smoothing it with a rolling window. This means defining a window around each timepoint ,calculating the mean of this window, and then repeating this for each timepoint.\n",
        "\n",
        "**Smoothing your data**\n",
        "\n",
        "For example, on the left we have a noisy timeseries as well as an overlay of seral small window Each timepoint will be replaced by the mean of the window just berfore it. The reuslt is a smoother signal over time which you can see on the right \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7_-lNFtisfU"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=18fSmfzfp4sbXtj_iix-VA83zL_Cvn7NV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKbvI3yrjp6f"
      },
      "source": [
        "**Calculating a rolling window statistic**\n",
        "\n",
        "We first use the dot-rolling method of our dataframe ,which retrun object that can be used to calculate many different statistics within each window. The window parameters tell us how many timepoints to include in each window. The larger the window , the smoother the result will be\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1VHPnfNJ1_K"
      },
      "source": [
        "# Audio is a Pandas DataFrame\n",
        "print(audio.shape)\n",
        "# (n_times, n_audio_files)\n",
        "#output (5000, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQvaqolKJVU"
      },
      "source": [
        "# Smooth our data by taking the rolling mean in a window of 50 samples\n",
        "window_size = 50\n",
        "windowed = audio.rolling(window=window_size)\n",
        "audio_smooth = windowed.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkn3buwrZTA2"
      },
      "source": [
        "**Calculating the auditory envelope**\n",
        "\n",
        "First we calculate the absolute value of each timepoint and this  is also called rectificatio becasue you ensure that all time point are postive. We calculate a rolling mean to smooth the singal "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw92SHOevuYP"
      },
      "source": [
        "First we take raw audio singal. Next we take the absolute of each time points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_gge7jKwr_s"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1rcEr7wxRDVcQTDSHFSQdS-MRnHfBimBi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxrSy5Nw11s"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1S-SdaFdN0sb6ZOUPLibd0wHG93or7RYx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8aOdzGzFnx"
      },
      "source": [
        "**Feature engineering the envelope**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY2pTG6TzJMD"
      },
      "source": [
        "# Calculate several features of the envelope, one per sound\n",
        "envelope_mean = np.mean(audio_envelope, axis=0)\n",
        "envelope_std = np.std(audio_envelope, axis=0)\n",
        "envelope_max = np.max(audio_envelope, axis=0)\n",
        "# Create our training data for a classifier\n",
        "X = np.column_stack([envelope_mean, envelope_std, envelope_max])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ILjFk1zNKy"
      },
      "source": [
        "**Preparing our features for scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rvHPmquzTRJ"
      },
      "source": [
        "X = np.column_stack([envelope_mean, envelope_std, envelope_max])\n",
        "y = labels.reshape([-1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQoXeRVQzk6d"
      },
      "source": [
        "**Cross validation for classication**\n",
        "\n",
        "cross_val_score automates the process of:\n",
        "- Splitting data into training / validation sets\n",
        "- Fitting the model on training data\n",
        "- Scoring it on validation data\n",
        "- Repeating this process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVNYO428QGu8"
      },
      "source": [
        "**Using cross_val_score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0z_yCkNQJrm"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "model = LinearSVC()\n",
        "scores = cross_val_score(model, X, y, cv=3)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xanyKYiVQgqS"
      },
      "source": [
        "**Auditory features: The Tempogram**\n",
        "\n",
        "- We can summarize more complex temporal information with timeseries-specic functions\n",
        "- librosa is a great library for auditory and timeseries feature engineering\n",
        "- Here we'll calculate the tempogram, which estimates the tempo of a sound over time\n",
        "- We can calculate summary statistics oftempo in the same way that we can for the envelope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emXHOr1aQ0U3"
      },
      "source": [
        "**Computing the tempogram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K3NPEkyQ3iM"
      },
      "source": [
        "# Import librosa and calculate the tempo of a 1-D sound array\n",
        "import librosa as lr\n",
        "audio_tempo = lr.beat.tempo(audio, sr=sfreq,\n",
        "hop_length=2**6, aggregate=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6oRa4OnZj5D"
      },
      "source": [
        "audio_rectified = audio.apply(np.abs)\n",
        "audio_envelope = audio_rectified.rolling(50).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLITTrxpSTeS"
      },
      "source": [
        "###**2.2.1  Calculating the envelope of sound**\n",
        "\n",
        "One of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You'll do this in the current exercise.\n",
        "\n",
        "A heartbeat file is available in the variable audio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8MNRchNVWGo"
      },
      "source": [
        "## Calculating the envelope of sound\n",
        "\n",
        "# Plot the raw data first\n",
        "audio.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRVnrXLwVL3S"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1DrljzjFqnwRYNZ7-D6zupqtoBOV3O_Ew)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bbgl4g1VnvF"
      },
      "source": [
        "# Rectify the audio signal\n",
        "audio_rectified = audio.apply(np.abs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvA6uBRUV9HY"
      },
      "source": [
        "# Plot the result\n",
        "audio_rectified.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RsOo4sFXoPv"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1oJhLSQR1qyumqgVNXaoXbmpPEbDZ3CDD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fa3KjTLY23a"
      },
      "source": [
        "# Smooth by applying a rolling mean\n",
        "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
        "\n",
        "# Plot the result\n",
        "audio_rectified_smooth.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drFbJ5QAatSa"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1Vc-bQYea6oouQgtV4Mmbn9L9ty8-3hvy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-otZrpcUcKz0"
      },
      "source": [
        "###**2.2.2 Calculating features from the envelope**\n",
        "\n",
        "Now that you've removed some of the noisier fluctuations in the audio, let's see if this improves your ability to classify.\n",
        "\n",
        "audio_rectified_smooth from the previous exercise is available in your workspace.\n",
        "\n",
        "**Instruction**\n",
        "\n",
        "- Calculate the mean, standard deviation, and maximum value for each heartbeat sound.\n",
        "- Column stack these stats in the same order.\n",
        "- Use cross-validation to fit a model on each CV iteration.\n",
        "\n",
        "**Hint**\n",
        "- You can use the mean(), std(), and max() functions from numpy and stack the outputs of each with column_stack.\n",
        "- Use cross_val_score() for cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV4Sc6vOIvsI"
      },
      "source": [
        "## Calculating features from the envelope\n",
        "# Calculate stats\n",
        "means = np.mean(audio_rectified_smooth, axis=0)\n",
        "stds = np.std(audio_rectified_smooth, axis=0)\n",
        "maxs = np.max(audio_rectified_smooth, axis=0)\n",
        "\n",
        "# Create the X and y arrays\n",
        "X = np.column_stack([means, stds, maxs])\n",
        "y = labels.reshape([-1, 1])\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "from sklearn.model_selection import cross_val_score\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXpovXbjgExJ"
      },
      "source": [
        "###**2.2.3 Derivative features: The tempogram**\n",
        "\n",
        "One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you'll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more.\n",
        "\n",
        "Note that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we'll access our Pandas data as a Numpy array with the .values attribute.\n",
        "\n",
        "**Instruction**\n",
        "\n",
        "- Use librosa to calculate a tempogram of each heartbeat audio.\n",
        "- Calculate the mean, standard deviation, and maximum of each tempogram (this time using DataFrame methods)\n",
        "\n",
        "**Hint**\n",
        "- The tempogram is calculated with the tempo function.\n",
        "- Numpy arrays have methods to calculate many statistics, such as array.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjwCBFalpi_j"
      },
      "source": [
        "## **2.3-The spectrogram**\n",
        "\n",
        "**Fourier transforms**\n",
        "\n",
        "- Timeseries data can be described as a combination of quickly-changing things and slowly-changing\n",
        "things\n",
        "- At each moment in time, we can describe the relative presence offast- and slow-moving components\n",
        "- The simplest way to do this is called a Fourier Transform\n",
        "- This converts a single timeseries into an array that describes the timeseries as a combination of\n",
        "oscillations\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ozCRe_IXBfbFgAAqjr15XA_5ZHiqpDxb)\n",
        "\n",
        "**Spectrograms:combinations of windows Fourier\n",
        "transforms** \n",
        "\n",
        "A spectrogram is a collection of windowed Fourier transforms over time\n",
        "Similar to how a rolling mean was calculated:\n",
        "1. Choose a window size and shape\n",
        "2. At a timepoint, calculate the FFT for that window\n",
        "3. Slide the window over by one\n",
        "4. Aggregate the results\n",
        "Called a Short-Time Fourier Transform (STFT)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Pihu41qWPxc98nabfyUjOHXiPGUKFZ9B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqP4dvvU0JFz"
      },
      "source": [
        "**Calculating the STFT**\n",
        "\n",
        "- We can calculate the STFT with librosa\n",
        "- There are several parameters we can tweak (such as window size)\n",
        "- For our purposes, we'll convert into decibels which normalizes the average values of all frequencies\n",
        "- We can then visualize it with the specshow() function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdEAgOB00qDR"
      },
      "source": [
        "**Calculating the STFT with code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeR25mXp0s6P"
      },
      "source": [
        "# Import the functions we'll use for the STFT\n",
        "from librosa.core import stft, amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "# Calculate our STFT\n",
        "HOP_LENGTH = 2**4\n",
        "SIZE_WINDOW = 2**7\n",
        "audio_spec = stft(audio, hop_length=HOP_LENGTH, n_fft=SIZE_WINDOW)\n",
        "# Convert into decibels for visualization\n",
        "spec_db = amplitude_to_db(audio_spec)\n",
        "# Visualize\n",
        "specshow(spec_db, sr=sfreq, x_axis='time',\n",
        "y_axis='hz', hop_length=HOP_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyumVHIBVnq2"
      },
      "source": [
        "**Spectral feature engineering**\n",
        "\n",
        "- Each timeseries has a different spectral pattern.\n",
        "- We can calculate these spectral patterns by analyzing the spectrogram.\n",
        "- For example, spectral bandwidth and spectral centroids describe where most ofthe energy is at each moment in time\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1OnpdMzI67K4XRaU0AF6W05bekwJ_IlVN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b72qo7RswOi"
      },
      "source": [
        "**Calculating spectral features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRBW_SRgszoZ"
      },
      "source": [
        "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
        "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
        "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
        "# Display these features on top of the spectrogram\n",
        "ax = specshow(spec, x_axis=\n",
        "'time'\n",
        ", y_axis=\n",
        "'hz'\n",
        ", hop_length=HOP_LENGTH)\n",
        "ax.plot(times_spec, centroids)\n",
        "ax.fill_between(times_spec, centroids - bandwidths / 2,\n",
        "centroids + bandwidths / 2, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DlYivCTtJUi"
      },
      "source": [
        "**Combining spectral and temporal features in a\n",
        "classier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU0vzB5etRaM"
      },
      "source": [
        "centroids_all = []\n",
        "bandwidths_all = []\n",
        "for spec in spectrograms:\n",
        "bandwidths = lr.feature.spectral_bandwidth(S=lr.db_to_amplitude(spec))\n",
        "centroids = lr.feature.spectral_centroid(S=lr.db_to_amplitude(spec))\n",
        "# Calculate the mean spectral bandwidth\n",
        "bandwidths_all.append(np.mean(bandwidths))\n",
        "# Calculate the mean spectral centroid\n",
        "centroids_all.append(np.mean(centroids))\n",
        "# Create our X matrix\n",
        "X = np.column_stack([means, stds, maxs, tempo_mean,\n",
        "tempo_max, tempo_std, bandwidths_all, centroids_all])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7tQzJZXIvsN"
      },
      "source": [
        "## Derivative features: The tempogram\n",
        "# Calculate the tempo of the sounds\n",
        "tempos = []\n",
        "for col, i_audio in audio.items():\n",
        "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
        "\n",
        "# Convert the list to an array so you can manipulate it more easily\n",
        "tempos = np.array(tempos)\n",
        "\n",
        "# Calculate statistics of each tempo\n",
        "tempos_mean = tempos.mean(axis=-1)\n",
        "tempos_std = tempos.std(axis=-1)\n",
        "tempos_max = tempos.max(axis=-1)\n",
        "________________________________________________________\n",
        "\n",
        "# Create the X and y arrays\n",
        "X = np.column_stack([means, stds, maxs, \n",
        "                     tempos_mean, \n",
        "                     tempos_std, \n",
        "                     tempos_max])\n",
        "\n",
        "y = labels.reshape([-1, 1])\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuJaYgAmh5oF"
      },
      "source": [
        "output:0.533333333333\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5vjjvQvIvsP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziq82VouIvsR"
      },
      "source": [
        "## Spectrograms of heartbeat audio\n",
        "\n",
        "# Import the stft function\n",
        "from librosa.core import stft\n",
        "\n",
        "# Prepare the STFT\n",
        "HOP_LENGTH = 2**4\n",
        "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
        "_____________________________________________________________\n",
        "\n",
        "from librosa.core import amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "\n",
        "# Convert into decibels\n",
        "spec_db = amplitude_to_db(spec)\n",
        "\n",
        "# Compare the raw audio to the spectrogram of the audio\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
        "axs[0].plot(time, audio)\n",
        "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21jQcM9IvsV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR3GI4QhIvsX"
      },
      "source": [
        "## Engineering spectral features\n",
        "\n",
        "import librosa as lr\n",
        "\n",
        "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
        "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
        "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
        "________________________________________________________________\n",
        "\n",
        "from librosa.core import amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "\n",
        "# Convert spectrogram to decibels for visualization\n",
        "spec_db = amplitude_to_db(spec)\n",
        "\n",
        "# Display these features on top of the spectrogram\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
        "ax.plot(times_spec, centroids)\n",
        "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
        "ax.set(ylim=[None, 6000])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGg3VxyoIvsa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdiiKb1-Ivse"
      },
      "source": [
        "## Combining many features in a classifier\n",
        "\n",
        "# Loop through each spectrogram\n",
        "bandwidths = []\n",
        "centroids = []\n",
        "\n",
        "for spec in spectrograms:\n",
        "    # Calculate the mean spectral bandwidth\n",
        "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
        "    # Calculate the mean spectral centroid\n",
        "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
        "    # Collect the values\n",
        "    bandwidths.append(this_mean_bandwidth)  \n",
        "    centroids.append(this_mean_centroid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foW1LMjkIvsg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RkI70o8Ivsj"
      },
      "source": [
        "# Create X and y arrays\n",
        "X = np.column_stack([means, stds, maxs,\n",
        "                     tempo_mean, \n",
        "                     tempo_max,\n",
        "                     tempo_std, \n",
        "                     bandwidths, centroids])\n",
        "y = labels.reshape([-1, 1])\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgKqSrcAIvsn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KiyensIvsr"
      },
      "source": [
        "3 - Predicting Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8oE4l70Ivss"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xor2GObSIvsx"
      },
      "source": [
        "## Introducing the dataset\n",
        "\n",
        "# Plot the raw values over time\n",
        "prices.plot()\n",
        "plt.show()\n",
        "_____________________________________________\n",
        "\n",
        "# Scatterplot with one company per axis\n",
        "prices.plot.scatter('EBAY', 'YHOO')\n",
        "plt.show()\n",
        "_____________________________________________\n",
        "\n",
        "# Scatterplot with color relating to time\n",
        "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
        "                    cmap=plt.cm.viridis, colorbar=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9jnidh4Ivs2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svcn6DoOIvs6"
      },
      "source": [
        "## Fitting a simple regression model\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Use stock symbols to extract training data\n",
        "X = all_prices[['EBAY', 'NVDA', 'YHOO']]\n",
        "y = all_prices[['AAPL']]\n",
        "\n",
        "# Fit and score the model with cross-validation\n",
        "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHc1aBdIvs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjyNMQCIvtB"
      },
      "source": [
        "## Visualizing predicted values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Split our data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    train_size=.8, shuffle=False, random_state=1)\n",
        "\n",
        "# Fit our model and generate predictions\n",
        "model = Ridge()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "score = r2_score(y_test, predictions)\n",
        "print(score)\n",
        "____________________________________________________\n",
        "\n",
        "# Visualize our predictions along with the \"true\" values, and print the score\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(y_test, color='k', lw=3)\n",
        "ax.plot(predictions, color='r', lw=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rgRnvAwIvtG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmu8RKdUIvtJ"
      },
      "source": [
        "## Visualizing messy data\n",
        "\n",
        "# Visualize the dataset\n",
        "prices.plot(legend=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count the missing values of each time series\n",
        "missing_values = prices.isna().sum()\n",
        "print(missing_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctRNp2U7IvtL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9LSDkSDIvtO"
      },
      "source": [
        "## Imputing missing values\n",
        "\n",
        "# Create a function we'll use to interpolate and plot\n",
        "def interpolate_and_plot(prices, interpolation):\n",
        "\n",
        "    # Create a boolean mask for missing values\n",
        "    missing_values = prices.isna()\n",
        "\n",
        "    # Interpolate the missing values\n",
        "    prices_interp = prices.interpolate(interpolation)\n",
        "\n",
        "    # Plot the results, highlighting the interpolated values in black\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
        "    \n",
        "    # Now plot the interpolated values on top in red\n",
        "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
        "    plt.show()\n",
        "__________________________________________________\n",
        "\n",
        "# Interpolate using the latest non-missing value\n",
        "interpolation_type = 'zero'\n",
        "interpolate_and_plot(prices, interpolation_type)\n",
        "\n",
        "__________________________________________________\n",
        "\n",
        "# Interpolate linearly\n",
        "interpolation_type = 'linear'\n",
        "interpolate_and_plot(prices, interpolation_type)\n",
        "__________________________________________________\n",
        "\n",
        "# Interpolate with a quadratic function\n",
        "interpolation_type = 'quadratic'\n",
        "interpolate_and_plot(prices, interpolation_type)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdUt9zBHIvtQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqGqHhBOIvtT"
      },
      "source": [
        "## Transforming raw data \n",
        "\n",
        "# Your custom function\n",
        "def percent_change(series):\n",
        "    # Collect all *but* the last value of this window, then the final value\n",
        "    previous_values = series[:-1]\n",
        "    last_value = series[-1]\n",
        "\n",
        "    # Calculate the % difference between the last value and the mean of earlier values\n",
        "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
        "    return percent_change\n",
        "\n",
        "# Apply your custom function and plot\n",
        "prices_perc = prices.rolling(20).apply(percent_change)\n",
        "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcnWFUvSIvtY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT0fDoUNIvtb"
      },
      "source": [
        "## Handling outliers\n",
        "\n",
        "def replace_outliers(series):\n",
        "    # Calculate the absolute difference of each timepoint from the series mean\n",
        "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
        "    \n",
        "    # Calculate a mask for the differences that are > 3 standard deviations from zero\n",
        "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
        "    \n",
        "    # Replace these values with the median accross the data\n",
        "    series[this_mask] = np.nanmedian(series)\n",
        "    return series\n",
        "\n",
        "# Apply your preprocessing function to the timeseries and plot the results\n",
        "prices_perc = prices_perc.apply(replace_outliers)\n",
        "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eunnsUfNIvth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TWJm9qeIvtk"
      },
      "source": [
        "## Engineering multiple rolling features at once\n",
        "\n",
        "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
        "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
        "\n",
        "# Define the features you'll calculate for each window\n",
        "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
        "\n",
        "# Calculate these features for your rolling window object\n",
        "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
        "\n",
        "# Plot the results\n",
        "ax = features.loc[:\"2011-01\"].plot()\n",
        "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
        "ax.legend(loc=(1.01, .6))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzH7inQpIvtq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_rCSN7ZIvts"
      },
      "source": [
        "## Percentiles and partial functions\n",
        "\n",
        "# Import partial from functools\n",
        "from functools import partial\n",
        "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
        "\n",
        "# Use a list comprehension to create a partial function for each quantile\n",
        "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
        "\n",
        "# Calculate each of these quantiles on the data using a rolling window\n",
        "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
        "features_percentiles = prices_perc_rolling.agg(percentile_functions)\n",
        "\n",
        "# Plot a subset of the result\n",
        "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
        "ax.legend(percentiles, loc=(1.01, .5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkxleHjpIvtu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQpm4tAiIvtx"
      },
      "source": [
        "## Using \"date\" information\n",
        "\n",
        "# Extract date features from the data, add them as columns\n",
        "prices_perc['day_of_week'] = prices_perc.index.weekday\n",
        "prices_perc['week_of_year'] = prices_perc.index.week\n",
        "prices_perc['month_of_year'] = prices_perc.index.month\n",
        "\n",
        "# Print prices_perc\n",
        "print(prices_perc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlh2Wzs2Ivt2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN5OkHuSIvt5"
      },
      "source": [
        "## Creating time-shifted features\n",
        "\n",
        "# These are the \"time lags\"\n",
        "shifts = np.arange(1, 11).astype(int)\n",
        "\n",
        "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
        "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
        "\n",
        "# Convert into a DataFrame for subsequent use\n",
        "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
        "\n",
        "# Plot the first 100 samples of each\n",
        "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
        "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
        "ax.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtrR9KpYIvt7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxH7D9awIvt9"
      },
      "source": [
        "## Special case: Auto-regressive models\n",
        "\n",
        "# Replace missing values with the median for each column\n",
        "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
        "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
        "\n",
        "# Fit the model\n",
        "model = Ridge()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwnH2TvoIvt_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf16m-jWIvuA"
      },
      "source": [
        "## Visualize regression coefficients\n",
        "\n",
        "def visualize_coefficients(coefs, names, ax):\n",
        "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
        "    ax.bar(names, coefs)\n",
        "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
        "    \n",
        "    # Set formatting so it looks nice\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "    return ax\n",
        "___________________________________________________________\n",
        "\n",
        "# Visualize the output data up to \"2011-01\"\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "y.loc[:'2011-01'].plot(ax=axs[0])\n",
        "\n",
        "# Run the function to visualize model's coefficients\n",
        "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgulk3MMIvuC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfVzqXBFIvuD"
      },
      "source": [
        "## Auto-regression with a smoother time series\n",
        "\n",
        "# Visualize the output data up to \"2011-01\"\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "y.loc[:'2011-01'].plot(ax=axs[0])\n",
        "\n",
        "# Run the function to visualize model's coefficients\n",
        "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnGHAGJiIvuH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOwedqOmIvuM"
      },
      "source": [
        "## Cross-validation with shuffling\n",
        "\n",
        "# Import ShuffleSplit and create the cross-validation object\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "cv = ShuffleSplit(10, random_state=1)\n",
        "\n",
        "# Iterate through CV splits\n",
        "results = []\n",
        "for tr, tt in cv.split(X, y):\n",
        "    # Fit the model on training data\n",
        "    model.fit(X[tr], y[tr])\n",
        "    \n",
        "    # Generate predictions on the test data, score the predictions, and collect\n",
        "    prediction = model.predict(X[tt])\n",
        "    score = r2_score(y[tt], prediction)\n",
        "    results.append((prediction, score, tt))\n",
        "\n",
        "# Custom function to quickly visualize predictions\n",
        "visualize_predictions(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKRbhKVEIvuP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJy6VtDIvuS"
      },
      "source": [
        "## Cross-validation without shuffling\n",
        "\n",
        "# Create KFold cross-validation object\n",
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
        "\n",
        "# Iterate through CV splits\n",
        "results = []\n",
        "for tr, tt in cv.split(X, y):\n",
        "    # Fit the model on training data\n",
        "    model.fit(X[tr], y[tr])\n",
        "    \n",
        "    # Generate predictions on the test data and collect\n",
        "    prediction = model.predict(X[tt])\n",
        "    results.append((prediction, tt))\n",
        "    \n",
        "# Custom function to quickly visualize predictions\n",
        "visualize_predictions(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVo5OHyiIvuV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zi9uOyPIvuX"
      },
      "source": [
        "## Time-based cross-validation\n",
        "\n",
        "# Import TimeSeriesSplit\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Create time-series cross-validation object\n",
        "cv = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# Iterate through CV splits\n",
        "fig, ax = plt.subplots()\n",
        "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
        "    # Plot the training data on each iteration, to see the behavior of the CV\n",
        "    ax.plot(tr, ii + y[tr])\n",
        "\n",
        "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C66hek_SIvud"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mNCjXPTIvuh"
      },
      "source": [
        "## Bootstrapping a confidence interval\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
        "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
        "    # Create our empty array to fill the results\n",
        "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
        "    for ii in range(n_boots):\n",
        "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
        "        random_sample = resample(data)\n",
        "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
        "        \n",
        "    # Compute the percentiles of choice for the bootstrapped means\n",
        "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
        "    return percentiles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4t293nlIvuk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is0Q11SRIvun"
      },
      "source": [
        "## Calculating variability in model coefficients\n",
        "\n",
        "# Iterate through CV splits\n",
        "n_splits = 100\n",
        "cv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Create empty array to collect coefficients\n",
        "coefficients = np.zeros([n_splits, X.shape[1]])\n",
        "\n",
        "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
        "    # Fit the model on training data and collect the coefficients\n",
        "    model.fit(X[tr], y[tr])\n",
        "    coefficients[ii] = model.coef_\n",
        "_______________________________________________________________\n",
        "    \n",
        "# Calculate a confidence interval around each coefficient\n",
        "bootstrapped_interval = bootstrap_interval(coefficients)\n",
        "\n",
        "# Plot it\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
        "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
        "ax.set(title='95% confidence interval for model coefficients')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbknkw-uIvup"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3rxMKohIvut"
      },
      "source": [
        "## Visualizing model score variability over time\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Generate scores for each split to see how the model performs over time\n",
        "scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
        "\n",
        "# Convert to a Pandas Series object\n",
        "scores_series = pd.Series(scores, index=times_scores, name='score')\n",
        "\n",
        "# Bootstrap a rolling confidence interval for the mean score\n",
        "scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\n",
        "scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n",
        "_________________________________________________________________\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
        "scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTBpa4R-Ivuv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu-WNeWrIvux"
      },
      "source": [
        "## Accounting for non-stationarity\n",
        "\n",
        "# Pre-initialize window sizes\n",
        "window_sizes = [25, 50, 75, 100]\n",
        "\n",
        "# Create an empty DataFrame to collect the stores\n",
        "all_scores = pd.DataFrame(index=times_scores)\n",
        "\n",
        "# Generate scores for each split to see how the model performs over time\n",
        "for window in window_sizes:\n",
        "    # Create cross-validation object using a limited lookback window\n",
        "    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
        "    \n",
        "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
        "    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
        "    all_scores['Length {}'.format(window)] = this_scores\n",
        "_______________________________________________________________\n",
        "\n",
        "# Visualize the scores\n",
        "ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
        "ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olPP_RTdIvuy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUwVOkxBZqGZ"
      },
      "source": [
        "#**References**\n",
        "\n",
        "[[1] Machine Learning for Time Series Data in Python](https://campus.datacamp.com/courses/machine-learning-for-time-series-data-in-python/time-series-and-machine-learning-primer?ex=1)\n",
        "\n",
        "[ [2] Time Series Analysis in Python | Time Series Forecasting | Data Science with Python | Edureka](https://www.youtube.com/watch?v=e8Yw4alG16Q&feature=youtu.be?utm_source=facebook&utm_medium=crosspost&utm_campaign=social-media-080920-mw&fbclid=IwAR0ChObyKJt5r5wctXpn_7twZfI6wfDIzWSfv3O3lCYPL67FcCT8g2J-wXs)"
      ]
    }
  ]
}