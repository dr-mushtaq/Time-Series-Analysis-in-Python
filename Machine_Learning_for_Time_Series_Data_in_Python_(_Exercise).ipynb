{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf-gpu-2.0",
      "language": "python",
      "name": "tf_gpu2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Machine_Learning_for_Time_Series_Data_in_Python_(_Exercise).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Time-Series-Analysis-in-Python/blob/master/Machine_Learning_for_Time_Series_Data_in_Python_(_Exercise).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLpgOW1eIvq0",
        "colab_type": "raw"
      },
      "source": [
        "1 - Time Series and Machine Learning Primer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U2u5S2IIvq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Time-Series-Analysis-in-Python.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8im90Q8skxno",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "28fa658a-7573-4c02-9f42-4733f4b894f7"
      },
      "source": [
        "# this code is used to upload dataset from Pc to colab\n",
        "from google.colab import files # Please First run this cod in chrom \n",
        "def getLocalFiles():\n",
        "    _files = files.upload() # upload StudentNextSessionf.csv datase\n",
        "    if len(_files) >0: # Then run above  libray \n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0409be0d-272a-4f52-b47f-5895415c068b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0409be0d-272a-4f52-b47f-5895415c068b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_cases1.csv to data_cases1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VEjJRNCk2ua",
        "colab_type": "text"
      },
      "source": [
        "# **Important libray**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5O4oYyplCsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzpVoEZelN_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('data_cases1.csv', usecols=['Date','Confirmed'],index_col='Date', parse_dates=True )\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg2QmYCfkBOk",
        "colab_type": "text"
      },
      "source": [
        "#**1-Time Series and Machine Learning Primer**\n",
        "This chapter is an introduction to the basics of machine learning, time series data, and the intersection between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwnIbf9qkKxN",
        "colab_type": "text"
      },
      "source": [
        "## **1.1 Plotting a time series (1)**\n",
        "\n",
        "In this exercise, you'll practice plotting the values of two time series without the time component.\n",
        "Two DataFrames, data and data2 are available in your workspace.\n",
        "\n",
        "Unless otherwise noted, assume that all required packages are loaded with their common aliases throughout this course.\n",
        "\n",
        "Note: This course assumes some familiarity with time series data, as well as how to use them in data analytics pipelines. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgm_ekOZl_Qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the first 5 rows of data\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNGxKkhiIvq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the time series in each dataset\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
        "data.iloc[:1000].plot(y='data_values', ax=axs[0])\n",
        "data2.iloc[:1000].plot(y='data_values', ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9EFJEv9yeD",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ESb3vBxskUApTHJGXwbbuvyjOrSVVFdw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiTGnlbtvbZP",
        "colab_type": "text"
      },
      "source": [
        "## **1.2 Plotting a time series (II)**\n",
        "\n",
        "You'll now plot both the datasets again, but with the included time stamps for each (stored in the column called \"time\"). Let's see if this gives you some more context for understanding each time series data.\n",
        "\n",
        "Plot data and data2 on top of one another, one per axis object.\n",
        "\n",
        "The x-axis should represent the time stamps and the y-axis should represent the dataset values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pQNfc-1Ivq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the time series in each dataset\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
        "data.iloc[:1000].plot(x='time', y='data_values', ax=axs[0])\n",
        "data.iloc[:1000].plot(x='time', y='data_values', ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEEowo3G_Ak5",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1HetuK2uXmyfNsWOA7Tg0iWAqvvPwG-_z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NPuMA2dMWuW",
        "colab_type": "text"
      },
      "source": [
        "## **1.3-Fitting a simple model: classification**\n",
        "\n",
        "**Scikit-learn**\n",
        "\n",
        "is most popular libary of machine leanring \n",
        "\n",
        "**Preparing data for scikit-learn**\n",
        "- scikit-learn expects a particular structure of data:\n",
        "(samples, features)\n",
        "- Make sure that your data is atleasttwo-dimensiona\n",
        "- Make sure the rst dimension is samples\n",
        "\n",
        "**If your data is not shaped properly**\n",
        "\n",
        "Ifthe axes are swapped:\n",
        "- array.T.shape\n",
        "\n",
        "**If your data is not shaped properly**\n",
        "\n",
        "If we're missing an axis, use .reshape() :\n",
        "- array.shape\n",
        "- (10,)\n",
        "- rray.reshape([-1, 1]).shape\n",
        "- (10, 1)\n",
        "- -1 will automatically llthat axis with remaining values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-_QOyLQEn6",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, you'll use the iris dataset (representing petal characteristics of a number of flowers) to practice using the scikit-learn API to fit a classification model. You can see a sample plot of the data to the right.\n",
        "\n",
        "Extract the \"petal length (cm)\" and \"petal width (cm)\" columns of data and assign it to X.\n",
        "\n",
        "Fit a model on X and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amjc1WnlIvrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the first 5 rows for inspection\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnNOlBbAIvrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "# Construct data for the model\n",
        "X = data[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = data[['target']]\n",
        "\n",
        "# Fit the model\n",
        "model = LinearSVC()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbGIecTKNpWB",
        "colab_type": "text"
      },
      "source": [
        "## **1.4 Predicting using a classification model**\n",
        "\n",
        "Now that you have fit your classifier, let's use it to predict the type of flower (or class) for some newly-collected flowers.\n",
        "\n",
        "Information about petal width and length for several new flowers is stored in the variable targets. Using the classifier you fit, you'll predict the type of each flower.\n",
        "\n",
        "Predict the flower type using the array X_predict.\n",
        "Run the given code to visualize the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K5fNIaHIvrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create input array\n",
        "X_predict = targets[['petal length (cm)', 'petal width (cm)']]\n",
        "\n",
        "# Predict with the model\n",
        "predictions = model.predict(X_predict)\n",
        "print(predictions)\n",
        "\n",
        "# Visualize predictions and actual values\n",
        "plt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n",
        "            c=predictions, cmap=plt.cm.coolwarm)\n",
        "plt.title(\"Predicted class values\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf1G9JIXPuAu",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1TcY2C2h-JI5X2Rs2wKLVnLY9hnnApZDu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3FA0-1oP6uF",
        "colab_type": "text"
      },
      "source": [
        "## **1.5-Fitting a simple model: regression**\n",
        "\n",
        "In this exercise, you'll practice fitting a regression model using data from the Boston housing market. A DataFrame called boston is available in your workspace. It contains many variables of data (stored as columns). Can you find a relationship between the following two variables?\n",
        "\n",
        "- \"AGE\": proportion of owner-occupied units built prior to 1940\n",
        "- \"RM\" : average number of rooms per dwelling\n",
        "\n",
        "Prepare X and y DataFrames using the data in boston.\n",
        "X should be the proportion of houses built prior to 1940, y average number of rooms per dwelling.\n",
        "Fit a regression model that uses these variables (remember to shape the variables correctly!).\n",
        "Don't forget that each variable must be the correct shape for scikit-learn to use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpDrBD7IIvrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "# Prepare input and output DataFrames\n",
        "X = boston[['AGE']]\n",
        "y = boston[['RM']]\n",
        "# Fit the model\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CZiNxkkRdfQ",
        "colab_type": "text"
      },
      "source": [
        "## **1.6 Predicting using a regression model**\n",
        "\n",
        "Now that you've fit a model with the Boston housing data, lets see what predictions it generates on some new data. You can investigate the underlying relationship that the model has found between inputs and outputs by feeding in a range of numbers as inputs and seeing what the model predicts for each input.\n",
        "\n",
        "A 1-D array new_inputs consisting of 100 \"new\" values for \"AGE\" (proportion of owner-occupied units built prior to 1940) is available in your workspace along with the model you fit in the previous exercise.\n",
        "\n",
        "Review new_inputs in the shell.\n",
        "Reshape new_inputs appropriately to generate predictions.\n",
        "Run the given code to visualize the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgwo-0xgIvrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions with the model using those inputs\n",
        "predictions = model.predict(new_inputs.reshape(-1,1))\n",
        "# Visualize the inputs and predicted values\n",
        "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
        "plt.xlabel('inputs')\n",
        "plt.ylabel('predictions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InnbPcDSSdQ",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1flKX2T_99HNRO_soOf5RgFnjP6bJs6_o)\n",
        "\n",
        "Here the red line shows the relationship that your model found. As the proportion of pre-1940s houses gets larger, the average number of rooms gets slightly lower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr1CtH3aazAR",
        "colab_type": "text"
      },
      "source": [
        "## **1.7 Machine learning and time series data**\n",
        "\n",
        "**Getting to know our data**\n",
        "- The datasets that we'll use in this course are all freely-available online\n",
        "- There are many datasets available to download on the web,the ones we'll use come from Kaggle\n",
        "\n",
        "**The Heartbeat Acoustic Data**\n",
        "\n",
        "- Many recordings of heart sounds from different patients\n",
        "- Some had normally-functioning hearts, others had abnormalities\n",
        "- Data comes in the form of audio les + labels for each le\n",
        "- Can we nd the \"abnormal\" heart beats?\n",
        "\n",
        "**Loading auditory data**\n",
        "\n",
        "Audo data mostly store in way file. The glob function is used to listen this file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jK2IsDzIvrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "files = glob('data/heartbeat-sounds/files/*.wav')\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdWQ0OQNdiML",
        "colab_type": "text"
      },
      "source": [
        "### **1.7.1 Reading in auditory data**\n",
        "\n",
        "we used **librosa** read the audio dataset and it also have function to extraction features, visiluzation and analysis for audio data. The data is load through **load** function. The data are stroe in **audo** and **sfreq** . The sfreq is 2205 which means there is 2205 samples recoreded per second "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1GkQs5ofABR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa as lr\n",
        "# `load` accepts a path to an audio file\n",
        "audio, sfreq = lr.load('data/heartbeat-sounds/proc/files/murmur__201101051104.wav')\n",
        "print(sfreq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs4-JCUVfvSS",
        "colab_type": "text"
      },
      "source": [
        "###**1.7.2-Inferring time from samples**\n",
        "\n",
        "- If we know the sampling rate of a timeseries,then we know the timestamp of each datapoint relative\n",
        "to the rst datapoint\n",
        "- Note:this assumes the sampling rate is xed and no data points are lost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ne9sv60gt0S",
        "colab_type": "text"
      },
      "source": [
        "### **1.7.3 Creating a time array (I)**\n",
        "\n",
        "Now we create array of timestammp for data . to do so you have two option 1) Generate array of indice from zero to number of data point in your audio file, divide each indix with sampling frequencey and finally you will get timepoint for each data point "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybfzgsHah8jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = np.arange(0, len(audio))\n",
        "time = indices / sfreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWNBDBc0Ivrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Inspecting the classification data\n",
        "\n",
        "import librosa as lr\n",
        "from glob import glob\n",
        "\n",
        "# List all the wav files in the folder\n",
        "audio_files = glob(data_dir + '/*.wav')\n",
        "\n",
        "# Read in the first audio file, create the time array\n",
        "audio, sfreq = lr.load(audio_files[0])\n",
        "time = np.arange(0, len(audio)) / sfreq\n",
        "\n",
        "# Plot audio over time\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(time, audio)\n",
        "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
        "plt.show()\n",
        "_________________________________\n",
        "\n",
        "# A common procedure in machine learning is to separate the\n",
        "# datapoints with lots of stuff happening from the ones that don't."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnDsHodOIvri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9Y2P5bKIvrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Inspecting the regression data\n",
        "\n",
        "# Read in the data\n",
        "data = pd.read_csv('prices.csv', index_col=0)\n",
        "\n",
        "# Convert the index of the DataFrame to datetime\n",
        "data.index = pd.to_datetime(data.index)\n",
        "print(data.head())\n",
        "________________________________________\n",
        "\n",
        "# Loop through each column, plot its values over time\n",
        "fig, ax = plt.subplots()\n",
        "for column in data.columns:\n",
        "    data[column].plot(ax=ax, label=column)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Anr8uESIvro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbx8AZKjIvrr",
        "colab_type": "raw"
      },
      "source": [
        "2 - Time Series as Inputs to a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh4J2thVIvrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## SFREQ = sampling frequency "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK5mEXeKIvru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KowOa-kKIvrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Many repetitions of sounds\n",
        "\n",
        "fig, axs = plt.subplots(3, 2, figsize=(15, 7), \n",
        "                        sharex=True, sharey=True)\n",
        "\n",
        "# Calculate the time array\n",
        "time = np.arange(0, len(normal)) / sfreq\n",
        "\n",
        "# Stack the normal/abnormal audio so you can loop and plot\n",
        "stacked_audio = np.hstack([normal, abnormal]).T\n",
        "\n",
        "# Loop through each audio file / ax object and plot\n",
        "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
        "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
        "    ax.plot(time, iaudio)\n",
        "show_plot_and_make_titles()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhX-AwwoIvrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctDdF258Ivr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Invariance in time\n",
        "\n",
        "# Average across the audio files of each DataFrame\n",
        "mean_normal = np.mean(normal, axis=1)\n",
        "mean_abnormal = np.mean(abnormal, axis=1)\n",
        "\n",
        "# Plot each average over time\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
        "ax1.plot(time, mean_normal)\n",
        "ax1.set(title=\"Normal Data\")\n",
        "ax2.plot(time, mean_abnormal)\n",
        "ax2.set(title=\"Abnormal Data\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaia3fDwIvr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaGuZDfGIvr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Build a classification model\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Initialize and fit the model\n",
        "model = LinearSVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions and score them manually\n",
        "predictions = model.predict(X_test)\n",
        "print(sum(predictions == y_test.squeeze()) / len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyYDyFJEIvr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDePTxjMIvsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calculating the envelope of sound\n",
        "\n",
        "# Plot the raw data first\n",
        "audio.plot(figsize=(10, 5))\n",
        "plt.show()\n",
        "_________________________________________________\n",
        "\n",
        "# Rectify the audio signal\n",
        "audio_rectified = audio.apply(np.abs)\n",
        "\n",
        "# Plot the result\n",
        "audio_rectified.plot(figsize=(10, 5))\n",
        "plt.show()\n",
        "__________________________________________________\n",
        "\n",
        "# Smooth by applying a rolling mean\n",
        "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
        "\n",
        "# Plot the result\n",
        "audio_rectified_smooth.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrJUtT62IvsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV4Sc6vOIvsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calculating features from the envelope\n",
        "\n",
        "# Calculate stats\n",
        "means = np.mean(audio_rectified_smooth, axis=0)\n",
        "stds = np.std(audio_rectified_smooth, axis=0)\n",
        "maxs = np.max(audio_rectified_smooth, axis=0)\n",
        "\n",
        "# Create the X and y arrays\n",
        "X = np.column_stack([means, stds, maxs])\n",
        "y = labels.reshape([-1, 1])\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "from sklearn.model_selection import cross_val_score\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8sPZzMqIvsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7tQzJZXIvsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Derivative features: The tempogram\n",
        "\n",
        "# Calculate the tempo of the sounds\n",
        "tempos = []\n",
        "for col, i_audio in audio.items():\n",
        "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
        "\n",
        "# Convert the list to an array so you can manipulate it more easily\n",
        "tempos = np.array(tempos)\n",
        "\n",
        "# Calculate statistics of each tempo\n",
        "tempos_mean = tempos.mean(axis=-1)\n",
        "tempos_std = tempos.std(axis=-1)\n",
        "tempos_max = tempos.max(axis=-1)\n",
        "________________________________________________________\n",
        "\n",
        "# Create the X and y arrays\n",
        "X = np.column_stack([means, stds, maxs, \n",
        "                     tempos_mean, \n",
        "                     tempos_std, \n",
        "                     tempos_max])\n",
        "\n",
        "y = labels.reshape([-1, 1])\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5vjjvQvIvsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziq82VouIvsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Spectrograms of heartbeat audio\n",
        "\n",
        "# Import the stft function\n",
        "from librosa.core import stft\n",
        "\n",
        "# Prepare the STFT\n",
        "HOP_LENGTH = 2**4\n",
        "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
        "_____________________________________________________________\n",
        "\n",
        "from librosa.core import amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "\n",
        "# Convert into decibels\n",
        "spec_db = amplitude_to_db(spec)\n",
        "\n",
        "# Compare the raw audio to the spectrogram of the audio\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
        "axs[0].plot(time, audio)\n",
        "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21jQcM9IvsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR3GI4QhIvsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Engineering spectral features\n",
        "\n",
        "import librosa as lr\n",
        "\n",
        "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
        "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
        "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
        "________________________________________________________________\n",
        "\n",
        "from librosa.core import amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "\n",
        "# Convert spectrogram to decibels for visualization\n",
        "spec_db = amplitude_to_db(spec)\n",
        "\n",
        "# Display these features on top of the spectrogram\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
        "ax.plot(times_spec, centroids)\n",
        "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
        "ax.set(ylim=[None, 6000])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGg3VxyoIvsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdiiKb1-Ivse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Combining many features in a classifier\n",
        "\n",
        "# Loop through each spectrogram\n",
        "bandwidths = []\n",
        "centroids = []\n",
        "\n",
        "for spec in spectrograms:\n",
        "    # Calculate the mean spectral bandwidth\n",
        "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
        "    # Calculate the mean spectral centroid\n",
        "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
        "    # Collect the values\n",
        "    bandwidths.append(this_mean_bandwidth)  \n",
        "    centroids.append(this_mean_centroid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foW1LMjkIvsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RkI70o8Ivsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create X and y arrays\n",
        "X = np.column_stack([means, stds, maxs,\n",
        "                     tempo_mean, \n",
        "                     tempo_max,\n",
        "                     tempo_std, \n",
        "                     bandwidths, centroids])\n",
        "y = labels.reshape([-1, 1])\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgKqSrcAIvsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KiyensIvsr",
        "colab_type": "raw"
      },
      "source": [
        "3 - Predicting Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8oE4l70Ivss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xor2GObSIvsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Introducing the dataset\n",
        "\n",
        "# Plot the raw values over time\n",
        "prices.plot()\n",
        "plt.show()\n",
        "_____________________________________________\n",
        "\n",
        "# Scatterplot with one company per axis\n",
        "prices.plot.scatter('EBAY', 'YHOO')\n",
        "plt.show()\n",
        "_____________________________________________\n",
        "\n",
        "# Scatterplot with color relating to time\n",
        "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, \n",
        "                    cmap=plt.cm.viridis, colorbar=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9jnidh4Ivs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svcn6DoOIvs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Fitting a simple regression model\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Use stock symbols to extract training data\n",
        "X = all_prices[['EBAY', 'NVDA', 'YHOO']]\n",
        "y = all_prices[['AAPL']]\n",
        "\n",
        "# Fit and score the model with cross-validation\n",
        "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHc1aBdIvs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjyNMQCIvtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Visualizing predicted values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Split our data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    train_size=.8, shuffle=False, random_state=1)\n",
        "\n",
        "# Fit our model and generate predictions\n",
        "model = Ridge()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "score = r2_score(y_test, predictions)\n",
        "print(score)\n",
        "____________________________________________________\n",
        "\n",
        "# Visualize our predictions along with the \"true\" values, and print the score\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(y_test, color='k', lw=3)\n",
        "ax.plot(predictions, color='r', lw=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rgRnvAwIvtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmu8RKdUIvtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Visualizing messy data\n",
        "\n",
        "# Visualize the dataset\n",
        "prices.plot(legend=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count the missing values of each time series\n",
        "missing_values = prices.isna().sum()\n",
        "print(missing_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctRNp2U7IvtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9LSDkSDIvtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Imputing missing values\n",
        "\n",
        "# Create a function we'll use to interpolate and plot\n",
        "def interpolate_and_plot(prices, interpolation):\n",
        "\n",
        "    # Create a boolean mask for missing values\n",
        "    missing_values = prices.isna()\n",
        "\n",
        "    # Interpolate the missing values\n",
        "    prices_interp = prices.interpolate(interpolation)\n",
        "\n",
        "    # Plot the results, highlighting the interpolated values in black\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
        "    \n",
        "    # Now plot the interpolated values on top in red\n",
        "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
        "    plt.show()\n",
        "__________________________________________________\n",
        "\n",
        "# Interpolate using the latest non-missing value\n",
        "interpolation_type = 'zero'\n",
        "interpolate_and_plot(prices, interpolation_type)\n",
        "\n",
        "__________________________________________________\n",
        "\n",
        "# Interpolate linearly\n",
        "interpolation_type = 'linear'\n",
        "interpolate_and_plot(prices, interpolation_type)\n",
        "__________________________________________________\n",
        "\n",
        "# Interpolate with a quadratic function\n",
        "interpolation_type = 'quadratic'\n",
        "interpolate_and_plot(prices, interpolation_type)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdUt9zBHIvtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqGqHhBOIvtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Transforming raw data \n",
        "\n",
        "# Your custom function\n",
        "def percent_change(series):\n",
        "    # Collect all *but* the last value of this window, then the final value\n",
        "    previous_values = series[:-1]\n",
        "    last_value = series[-1]\n",
        "\n",
        "    # Calculate the % difference between the last value and the mean of earlier values\n",
        "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
        "    return percent_change\n",
        "\n",
        "# Apply your custom function and plot\n",
        "prices_perc = prices.rolling(20).apply(percent_change)\n",
        "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcnWFUvSIvtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT0fDoUNIvtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Handling outliers\n",
        "\n",
        "def replace_outliers(series):\n",
        "    # Calculate the absolute difference of each timepoint from the series mean\n",
        "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
        "    \n",
        "    # Calculate a mask for the differences that are > 3 standard deviations from zero\n",
        "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
        "    \n",
        "    # Replace these values with the median accross the data\n",
        "    series[this_mask] = np.nanmedian(series)\n",
        "    return series\n",
        "\n",
        "# Apply your preprocessing function to the timeseries and plot the results\n",
        "prices_perc = prices_perc.apply(replace_outliers)\n",
        "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eunnsUfNIvth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TWJm9qeIvtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Engineering multiple rolling features at once\n",
        "\n",
        "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
        "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
        "\n",
        "# Define the features you'll calculate for each window\n",
        "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
        "\n",
        "# Calculate these features for your rolling window object\n",
        "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
        "\n",
        "# Plot the results\n",
        "ax = features.loc[:\"2011-01\"].plot()\n",
        "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
        "ax.legend(loc=(1.01, .6))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzH7inQpIvtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_rCSN7ZIvts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Percentiles and partial functions\n",
        "\n",
        "# Import partial from functools\n",
        "from functools import partial\n",
        "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
        "\n",
        "# Use a list comprehension to create a partial function for each quantile\n",
        "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
        "\n",
        "# Calculate each of these quantiles on the data using a rolling window\n",
        "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
        "features_percentiles = prices_perc_rolling.agg(percentile_functions)\n",
        "\n",
        "# Plot a subset of the result\n",
        "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
        "ax.legend(percentiles, loc=(1.01, .5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkxleHjpIvtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQpm4tAiIvtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Using \"date\" information\n",
        "\n",
        "# Extract date features from the data, add them as columns\n",
        "prices_perc['day_of_week'] = prices_perc.index.weekday\n",
        "prices_perc['week_of_year'] = prices_perc.index.week\n",
        "prices_perc['month_of_year'] = prices_perc.index.month\n",
        "\n",
        "# Print prices_perc\n",
        "print(prices_perc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlh2Wzs2Ivt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN5OkHuSIvt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating time-shifted features\n",
        "\n",
        "# These are the \"time lags\"\n",
        "shifts = np.arange(1, 11).astype(int)\n",
        "\n",
        "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
        "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
        "\n",
        "# Convert into a DataFrame for subsequent use\n",
        "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
        "\n",
        "# Plot the first 100 samples of each\n",
        "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
        "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
        "ax.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtrR9KpYIvt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxH7D9awIvt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Special case: Auto-regressive models\n",
        "\n",
        "# Replace missing values with the median for each column\n",
        "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
        "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
        "\n",
        "# Fit the model\n",
        "model = Ridge()\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwnH2TvoIvt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf16m-jWIvuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Visualize regression coefficients\n",
        "\n",
        "def visualize_coefficients(coefs, names, ax):\n",
        "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
        "    ax.bar(names, coefs)\n",
        "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
        "    \n",
        "    # Set formatting so it looks nice\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "    return ax\n",
        "___________________________________________________________\n",
        "\n",
        "# Visualize the output data up to \"2011-01\"\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "y.loc[:'2011-01'].plot(ax=axs[0])\n",
        "\n",
        "# Run the function to visualize model's coefficients\n",
        "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgulk3MMIvuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfVzqXBFIvuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Auto-regression with a smoother time series\n",
        "\n",
        "# Visualize the output data up to \"2011-01\"\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "y.loc[:'2011-01'].plot(ax=axs[0])\n",
        "\n",
        "# Run the function to visualize model's coefficients\n",
        "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnGHAGJiIvuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOwedqOmIvuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Cross-validation with shuffling\n",
        "\n",
        "# Import ShuffleSplit and create the cross-validation object\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "cv = ShuffleSplit(10, random_state=1)\n",
        "\n",
        "# Iterate through CV splits\n",
        "results = []\n",
        "for tr, tt in cv.split(X, y):\n",
        "    # Fit the model on training data\n",
        "    model.fit(X[tr], y[tr])\n",
        "    \n",
        "    # Generate predictions on the test data, score the predictions, and collect\n",
        "    prediction = model.predict(X[tt])\n",
        "    score = r2_score(y[tt], prediction)\n",
        "    results.append((prediction, score, tt))\n",
        "\n",
        "# Custom function to quickly visualize predictions\n",
        "visualize_predictions(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKRbhKVEIvuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJy6VtDIvuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Cross-validation without shuffling\n",
        "\n",
        "# Create KFold cross-validation object\n",
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
        "\n",
        "# Iterate through CV splits\n",
        "results = []\n",
        "for tr, tt in cv.split(X, y):\n",
        "    # Fit the model on training data\n",
        "    model.fit(X[tr], y[tr])\n",
        "    \n",
        "    # Generate predictions on the test data and collect\n",
        "    prediction = model.predict(X[tt])\n",
        "    results.append((prediction, tt))\n",
        "    \n",
        "# Custom function to quickly visualize predictions\n",
        "visualize_predictions(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVo5OHyiIvuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zi9uOyPIvuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Time-based cross-validation\n",
        "\n",
        "# Import TimeSeriesSplit\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Create time-series cross-validation object\n",
        "cv = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# Iterate through CV splits\n",
        "fig, ax = plt.subplots()\n",
        "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
        "    # Plot the training data on each iteration, to see the behavior of the CV\n",
        "    ax.plot(tr, ii + y[tr])\n",
        "\n",
        "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C66hek_SIvud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mNCjXPTIvuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Bootstrapping a confidence interval\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
        "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
        "    # Create our empty array to fill the results\n",
        "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
        "    for ii in range(n_boots):\n",
        "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
        "        random_sample = resample(data)\n",
        "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
        "        \n",
        "    # Compute the percentiles of choice for the bootstrapped means\n",
        "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
        "    return percentiles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4t293nlIvuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is0Q11SRIvun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calculating variability in model coefficients\n",
        "\n",
        "# Iterate through CV splits\n",
        "n_splits = 100\n",
        "cv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Create empty array to collect coefficients\n",
        "coefficients = np.zeros([n_splits, X.shape[1]])\n",
        "\n",
        "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
        "    # Fit the model on training data and collect the coefficients\n",
        "    model.fit(X[tr], y[tr])\n",
        "    coefficients[ii] = model.coef_\n",
        "_______________________________________________________________\n",
        "    \n",
        "# Calculate a confidence interval around each coefficient\n",
        "bootstrapped_interval = bootstrap_interval(coefficients)\n",
        "\n",
        "# Plot it\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
        "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
        "ax.set(title='95% confidence interval for model coefficients')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbknkw-uIvup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3rxMKohIvut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Visualizing model score variability over time\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Generate scores for each split to see how the model performs over time\n",
        "scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
        "\n",
        "# Convert to a Pandas Series object\n",
        "scores_series = pd.Series(scores, index=times_scores, name='score')\n",
        "\n",
        "# Bootstrap a rolling confidence interval for the mean score\n",
        "scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\n",
        "scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n",
        "_________________________________________________________________\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
        "scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTBpa4R-Ivuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu-WNeWrIvux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Accounting for non-stationarity\n",
        "\n",
        "# Pre-initialize window sizes\n",
        "window_sizes = [25, 50, 75, 100]\n",
        "\n",
        "# Create an empty DataFrame to collect the stores\n",
        "all_scores = pd.DataFrame(index=times_scores)\n",
        "\n",
        "# Generate scores for each split to see how the model performs over time\n",
        "for window in window_sizes:\n",
        "    # Create cross-validation object using a limited lookback window\n",
        "    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
        "    \n",
        "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
        "    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
        "    all_scores['Length {}'.format(window)] = this_scores\n",
        "_______________________________________________________________\n",
        "\n",
        "# Visualize the scores\n",
        "ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
        "ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olPP_RTdIvuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUwVOkxBZqGZ",
        "colab_type": "text"
      },
      "source": [
        "#**References**\n",
        "\n",
        "[[1] Machine Learning for Time Series Data in Python](https://campus.datacamp.com/courses/machine-learning-for-time-series-data-in-python/time-series-and-machine-learning-primer?ex=1)"
      ]
    }
  ]
}